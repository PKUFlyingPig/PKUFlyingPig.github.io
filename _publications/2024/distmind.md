---
title:          "DistMind: Efficient Resource Disaggregation for Deep Learning Workloads"
date:           2024-01-05 00:01:00 +0800
selected:       false
pub:            "Transactions on Networking (TON)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
    Deep learning (DL) systems suffer from low resource
    utilization due to (i) the monolithic server model that tightly
    couples compute and memory, and (ii) the limited sharing between
    different inference applications, and across inference and training,
    because of strict service level objectives (SLOs). To address
    this problem, we present DistMind, a disaggregated DL system
    that enables efficient multiplexing of DL applications with near-
    optimal resource utilization. DistMind decouples compute from
    host memory, and exposes the abstractions of a GPU pool and a
    memory pool, each of which can be independently provisioned.

authors:
  - Xin Jin
  - Zhihao Bai
  - Zhen Zhang
  - Yibo Zhu
  - Yinmin Zhong
  - Xuanzhe Liu

links:
  Paper: https://ieeexplore.ieee.org/document/10414009
---
