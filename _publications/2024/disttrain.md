---
title:          "DistTrain: Addressing Model and Data Heterogeneity with Disaggregated Training for Multimodal Large Language Models"
date:           2024-08-15 00:01:00 +0800
selected:       false
pub:            "In preprint"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
    This work presents DistTrain, an efficient and adaptive framework to reform the training of multimodal large language models on large-scale clusters. The core of DistTrain is the disaggregated training technique that exploits the characteristics of multimodal LLM training to achieve high efficiency and scalability. Specifically, it leverages disaggregated model orchestration and disaggregated data reordering to address model and data heterogeneity respectively.

cover:    /assets/images/covers/disttrain.png
authors:
  - Zili Zhang
  - Yinmin Zhong
  - Ranchen Ming
  - Hanpeng Hu
  - Jianjian Sun
  - Zheng Ge
  - Yibo Zhu
  - Xin Jin

links:
  Paper: https://arxiv.org/pdf/2408.04275
---
