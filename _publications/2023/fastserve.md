---
title:          "Fast Distributed Inference Serving for Large Language Models"
date:           2023-04-20 00:01:00 +0800
selected:       false
pub:            "In preprint"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2024"

abstract: >-
    This paper presents FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler.

# cover:
authors:
  - Bingyang Wu*
  - Yinmin Zhong*
  - Zili Zhang*
  - Gang Huang
  - Xuanzhe Liu
  - Xin Jin

links:
  Paper: https://arxiv.org/pdf/2305.05920.pdf
---
